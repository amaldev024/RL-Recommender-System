{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 08:29:44.453771: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-07 08:29:44.453823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-07 08:29:44.455774: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-07 08:29:44.466681: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-07 08:29:45.672441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from collections import deque\n",
    "from utility_v2 import *\n",
    "from trfl import indexing_ops\n",
    "import trfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "data = \"abc\"\n",
    "batch_size = 256\n",
    "hidden_factor = 62\n",
    "r_click = 1\n",
    "r_buy = 5\n",
    "lr = 0.01\n",
    "discount = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, learning_rate, item_num, state_size, pretrain, model_name='DQNetwork'):\n",
    "        super(QNetwork, self).__init__(name=model_name)\n",
    "        # self.state_size = state_size\n",
    "        # learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.item_num = int(item_num)\n",
    "        self.pretrain = pretrain\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.all_embeddings = tf.keras.layers.Embedding(input_dim=self.item_num + 1, output_dim=self.hidden_size, name='all_embeddings')\n",
    "        self.gru = tf.keras.layers.GRU(self.hidden_size, name='gru')\n",
    "\n",
    "        self.output1 = tf.keras.layers.Dense(self.item_num, activation=None, name=\"q-value\")\n",
    "        self.output2 = tf.keras.layers.Dense(self.item_num, activation=None, name=\"ce-logits\")\n",
    "\n",
    "    def call(self, inputs, len_state=10):\n",
    "        inputs_tensor = tf.convert_to_tensor(inputs)\n",
    "        input_emb = self.all_embeddings(inputs_tensor)\n",
    "        states_hidden = self.gru(input_emb)\n",
    "\n",
    "        q_values = self.output1(states_hidden)\n",
    "        ce_logits = self.output2(states_hidden)\n",
    "        \n",
    "        return q_values, ce_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(mainQN):\n",
    "    \n",
    "    print(\"Entered evaluate part\")\n",
    "    \n",
    "    eval_sessions = pd.read_pickle(os.path.join(data_directory, 'sampled_val.df'))\n",
    "    eval_ids = eval_sessions.session_id.unique()\n",
    "    groups = eval_sessions.groupby('session_id')\n",
    "    batch = 100\n",
    "    evaluated = 0\n",
    "    total_clicks = 0.0\n",
    "    total_purchase = 0.0\n",
    "    total_reward = [0, 0, 0, 0]\n",
    "    hit_clicks = [0, 0, 0, 0]\n",
    "    ndcg_clicks = [0, 0, 0, 0]\n",
    "    hit_purchase = [0, 0, 0, 0]\n",
    "    ndcg_purchase = [0, 0, 0, 0]\n",
    "    \n",
    "    while evaluated < len(eval_ids):\n",
    "        states, len_states, actions, rewards = [], [], [], []\n",
    "        for i in range(batch):\n",
    "            id = eval_ids[evaluated]\n",
    "            group = groups.get_group(id)\n",
    "            history = []\n",
    "            for index, row in group.iterrows():\n",
    "                state = list(history)\n",
    "                len_states.append(state_size if len(state) >= state_size else 1 if len(state) == 0 else len(state))\n",
    "                state = pad_history(state, state_size, item_num)\n",
    "                states.append(state)\n",
    "                action = row['item_id']\n",
    "                is_buy = row['is_buy']\n",
    "                reward = reward_buy if is_buy == 1 else reward_click\n",
    "                if is_buy == 1:\n",
    "                    total_purchase += 1.0\n",
    "                else:\n",
    "                    total_clicks += 1.0\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                history.append(row['item_id'])\n",
    "            evaluated += 1\n",
    "\n",
    "        _, prediction = mainQN(states)\n",
    "        sorted_list = np.argsort(prediction)\n",
    "        calculate_hit(sorted_list, topk, actions, rewards, reward_click, total_reward, hit_clicks, ndcg_clicks, hit_purchase, ndcg_purchase)\n",
    "    \n",
    "    print('#############################################################')\n",
    "    print('total clicks: %d, total purchase:%d' % (total_clicks, total_purchase))\n",
    "    for i in range(len(topk)):\n",
    "        hr_click = hit_clicks[i] / total_clicks\n",
    "        hr_purchase = hit_purchase[i] / total_purchase\n",
    "        ng_click = ndcg_clicks[i] / total_clicks\n",
    "        ng_purchase = ndcg_purchase[i] / total_purchase\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print('cumulative reward @ %d: %f' % (topk[i], total_reward[i]))\n",
    "        print('clicks hr ndcg @ %d : %f, %f' % (topk[i], hr_click, ng_click))\n",
    "        print('purchase hr and ndcg @%d : %f, %f' % (topk[i], hr_purchase, ng_purchase))\n",
    "    print('#############################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QNetwork(tf.keras.Model):\n",
    "#     def __init__(self, hidden_size, learning_rate, item_num, state_size, pretrain, model_name='DQNetwork'):\n",
    "#         super(QNetwork, self).__init__(name=model_name)\n",
    "#         # self.state_size = state_size\n",
    "#         # self.learning_rate = learning_rate\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.item_num = int(item_num)\n",
    "#         self.pretrain = pretrain\n",
    "#         self.model_name = model_name\n",
    "\n",
    "#         self.all_embeddings = tf.keras.layers.Embedding(input_dim=self.item_num + 1, output_dim=self.hidden_size, name='all_embeddings')\n",
    "#         self.gru = tf.keras.layers.GRU(self.hidden_size, name='gru')\n",
    "\n",
    "\n",
    "#         # self.inputs = tf.keras.layers.Input(shape=(state_size,), dtype=tf.int32)\n",
    "#         # self.len_state = tf.keras.layers.Input(shape=(), dtype=tf.int32)\n",
    "#         # self.input_emb = tf.keras.layers.Embedding(input_dim=self.item_num + 1, output_dim=self.hidden_size)(self.inputs)\n",
    "#         # gru_out, self.states_hidden = tf.keras.layers.GRU(self.hidden_size)(self.input_emb, mask=tf.sequence_mask(self.len_state, dtype=tf.float32))\n",
    "\n",
    "#         self.output1 = tf.keras.layers.Dense(self.item_num, activation=None, name=\"q-value\")(self.states_hidden)\n",
    "#         self.output2 = tf.keras.layers.Dense(self.item_num, activation=None, name=\"ce-logits\")(self.states_hidden)\n",
    "\n",
    "#         # self.actions = tf.keras.layers.Input(shape=(), dtype=tf.int32)\n",
    "#         # self.targetQs_ = tf.keras.layers.Input(shape=(item_num,), dtype=tf.float32)\n",
    "#         # self.targetQs_selector = tf.keras.layers.Input(shape=(item_num,), dtype=tf.float32)\n",
    "#         # self.reward = tf.keras.layers.Input(shape=(), dtype=tf.float32)\n",
    "#         # self.discount = tf.keras.layers.Input(shape=(), dtype=tf.float32)\n",
    "\n",
    "#         def forward(self, inputs, len_state):\n",
    "#             inputs = tf.convert_to_tensor(inputs)\n",
    "#             input_emb = self.all_embeddings(inputs)\n",
    "#             states_hidden = self.gru(input_emb)\n",
    "\n",
    "#             q_values = self.output1(states_hidden)\n",
    "#             ce_logits = self.output2(states_hidden)\n",
    "\n",
    "#             # qloss, q_learning = trfl.double_qlearning(Q_values, self.actions, self.reward, self.discount, self.targetQs_, self.targetQs_selector)\n",
    "#             # q_indexed = tf.gather(Q_values, self.actions, axis=1, batch_dims=0)\n",
    "#             # celoss = tf.multiply(q_indexed, tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.actions, logits=ce_logits))\n",
    "#             # loss = tf.reduce_mean(qloss + celoss)\n",
    "            \n",
    "#             return ce_logits, q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = r'E:\\MIni\\v2\\data'\n",
    "data_directory = '/media/amd/New Volume/MIni/v2/data'\n",
    "data_statis = pd.read_pickle(os.path.join(data_directory, 'data_statis.df'))\n",
    "state_size = data_statis['state_size'][0]\n",
    "item_num = data_statis['item_num'][0]\n",
    "reward_click = r_click\n",
    "reward_buy = r_buy\n",
    "topk = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 08:29:48.628117: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "QN_1 = QNetwork(hidden_size=hidden_factor, learning_rate=lr, item_num=item_num,\n",
    "                    state_size=state_size, pretrain=False)\n",
    "QN_2 = QNetwork(hidden_size=hidden_factor, learning_rate=lr, item_num=item_num,\n",
    "                    state_size=state_size, pretrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n",
      "it works\n",
      "aaaaaaait works\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(target_Qs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_done[index]:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# target_Qs[index] = np.zeros(item_num)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m         target_Qs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtensor_scatter_nd_update(target_Qs, tf\u001b[38;5;241m.\u001b[39mexpand_dims(tf\u001b[38;5;241m.\u001b[39mrange(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_Qs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), tf\u001b[38;5;241m.\u001b[39mzeros([tf\u001b[38;5;241m.\u001b[39mshape(target_Qs)[\u001b[38;5;241m0\u001b[39m], item_num]))\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# print(\"current_Qs:\", current_Qs.dtype, current_Qs.shape)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# print(\"action:\", action.dtype, action.shape)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# print(\"reward:\", reward.dtype, reward.shape)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# print(\"discount:\", discount.dtype, discount.shape)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# print(\"target_Qs:\", target_Qs.dtype, target_Qs.shape)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# print(\"target_Qs_selector:\", target_Qs_selector.dtype, target_Qs_selector.shape)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m qloss, q_learning \u001b[38;5;241m=\u001b[39m trfl\u001b[38;5;241m.\u001b[39mdouble_qlearning(current_Qs, action, reward, discounts,\n\u001b[1;32m     69\u001b[0m                                           target_Qs, target_Qs_selector)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:656\u001b[0m, in \u001b[0;36mshape_v2\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m    654\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     out_type \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mint32\n\u001b[0;32m--> 656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:688\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    686\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     out_type \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mint32\n\u001b[0;32m--> 688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshape_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:710\u001b[0m, in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape_internal\u001b[39m(\u001b[38;5;28minput\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, out_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the shape of a tensor.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m  If `out_type` is not specified and the shape is fully known, then we look at\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;28minput\u001b[39m, (sparse_tensor\u001b[38;5;241m.\u001b[39mSparseTensor, sparse_tensor\u001b[38;5;241m.\u001b[39mSparseTensorValue)):\n\u001b[1;32m    713\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_type:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:5430\u001b[0m, in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values, skip_on_eager)\u001b[0m\n\u001b[1;32m   5405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname_scope\u001b[39m(name, default_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, skip_on_eager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   5406\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Internal-only entry point for `name_scope*`.\u001b[39;00m\n\u001b[1;32m   5407\u001b[0m \n\u001b[1;32m   5408\u001b[0m \u001b[38;5;124;03m  Internal ops do not use the public API and instead rely on\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5428\u001b[0m \u001b[38;5;124;03m    `name_scope*` context manager.\u001b[39;00m\n\u001b[1;32m   5429\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5430\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   5431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m internal_name_scope_v1(name, default_name, values)\n\u001b[1;32m   5433\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m skip_on_eager:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/context.py:2347\u001b[0m, in \u001b[0;36mexecuting_eagerly\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2293\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecuting_eagerly\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   2294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecuting_eagerly\u001b[39m():\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether the current thread has eager execution enabled.\u001b[39;00m\n\u001b[1;32m   2296\u001b[0m \n\u001b[1;32m   2297\u001b[0m \u001b[38;5;124;03m  Eager execution is enabled by default and this API returns `True`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;124;03m    `True` if the current thread has eager execution enabled.\u001b[39;00m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2347\u001b[0m   ctx \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2348\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_execution_mode \u001b[38;5;241m==\u001b[39m EAGER_MODE\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/context.py:2265\u001b[0m, in \u001b[0;36mcontext_safe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontext_safe\u001b[39m():\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns current context (or None if one hasn't been initialized).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2265\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_context\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replay_buffer = pd.read_pickle(os.path.join(data_directory, 'replay_buffer.df'))\n",
    "total_step = 0\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "num_rows=replay_buffer.shape[0]\n",
    "num_batches=int(num_rows/batch_size)\n",
    "\n",
    "#current_q_val = tf.Variable(tf.zeros([batch_size, item_num]))\n",
    "\n",
    "# optimizer.build(mainQN.trainable_variables)\n",
    "optimizer.build(QN_1.trainable_variables + QN_2.trainable_variables)  # Build optimizer with all trainable variables\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(num_batches):\n",
    "        batch = replay_buffer.sample(n=batch_size).to_dict()\n",
    "        \n",
    "        # print(batch.keys())\n",
    "        \n",
    "        state = list(batch['state'].values())\n",
    "        len_state = list(batch['len_state'].values())\n",
    "        \n",
    "        next_state = list(batch['next_state'].values())\n",
    "        len_next_state = list(batch['len_next_states'].values())\n",
    "        \n",
    "        action = list(batch['action'].values())\n",
    "        is_buy = list(batch['is_buy'].values())\n",
    "        reward = [reward_buy if is_buy[index] else reward_click for index in range(len(is_buy))]\n",
    "        \n",
    "        #This is giving raw value of discount need to change it to add a variable\n",
    "        discounts = [discount] * len(action)\n",
    "\n",
    "        pointer = np.random.randint(0, 2)\n",
    "        if pointer == 0:\n",
    "            mainQN = QN_1\n",
    "            target_QN = QN_2\n",
    "        else:\n",
    "            mainQN = QN_2\n",
    "            target_QN = QN_1\n",
    "        \n",
    "        action = tf.convert_to_tensor(action)  # Convert to TensorFlow tensor\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)  # Convert to TensorFlow tensor\n",
    "        # discount = tf.ones_like(reward) * discount\n",
    "        discounts = tf.convert_to_tensor(discounts)  # Convert to TensorFlow tensor\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            current_Qs, ce_logits = mainQN(state)\n",
    "            \n",
    "            target_Qs, _ = target_QN(next_state)\n",
    "            target_Qs_selector, _ = mainQN(next_state)\n",
    "            # print(target_Qs.shape, target_Qs_selector.shape, current_Qs.shape, ce_logits.shape, \"the shapes\")\n",
    "\n",
    "            is_done = list(batch['is_done'].values())\n",
    "            for index in range(target_Qs.shape[0]):\n",
    "                if is_done[index]:\n",
    "                    # target_Qs[index] = np.zeros(item_num)\n",
    "                    target_Qs = tf.tensor_scatter_nd_update(target_Qs, tf.expand_dims(tf.range(tf.shape(target_Qs)[0]), axis=1), tf.zeros([tf.shape(target_Qs)[0], item_num]))\n",
    "\n",
    "\n",
    "            # print(\"current_Qs:\", current_Qs.dtype, current_Qs.shape)\n",
    "            # print(\"action:\", action.dtype, action.shape)\n",
    "            # print(\"reward:\", reward.dtype, reward.shape)\n",
    "            # print(\"discount:\", discount.dtype, discount.shape)\n",
    "            # print(\"target_Qs:\", target_Qs.dtype, target_Qs.shape)\n",
    "            # print(\"target_Qs_selector:\", target_Qs_selector.dtype, target_Qs_selector.shape)\n",
    "            \n",
    "            qloss, q_learning = trfl.double_qlearning(current_Qs, action, reward, discounts,\n",
    "                                                      target_Qs, target_Qs_selector)\n",
    "            \n",
    "            q_indexed = tf.stop_gradient(indexing_ops.batched_index(current_Qs, action))\n",
    "            celoss = tf.multiply(q_indexed, tf.nn.sparse_softmax_cross_entropy_with_logits(labels=action,\n",
    "                                                                                           logits=ce_logits))\n",
    "            loss = tf.reduce_mean(qloss + celoss)\n",
    "\n",
    "        grads = tape.gradient(loss, mainQN.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, mainQN.trainable_variables))\n",
    "\n",
    "        total_step += 1\n",
    "        if total_step % 100 == 0:\n",
    "            print('Epoch {} - Step {} - Loss {}'.format(i, total_step, loss.numpy()))\n",
    "        if total_step % 1000 == 0:\n",
    "            QN_1.save_weights('DQN_1.h5')\n",
    "            QN_2.save_weights('DQN_2.h5')\n",
    "            evaluate(mainQN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
