{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 08:29:44.453771: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-07 08:29:44.453823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-07 08:29:44.455774: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-07 08:29:44.466681: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-07 08:29:45.672441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from collections import deque\n",
    "from utility_v2 import *\n",
    "from trfl import indexing_ops\n",
    "import trfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "data = \"abc\"\n",
    "batch_size = 256\n",
    "hidden_factor = 62\n",
    "r_click = 1\n",
    "r_buy = 5\n",
    "lr = 0.01\n",
    "discount = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, learning_rate, item_num, state_size, pretrain, model_name='DQNetwork'):\n",
    "        super(QNetwork, self).__init__(name=model_name)\n",
    "        # self.state_size = state_size\n",
    "        # learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.item_num = int(item_num)\n",
    "        self.pretrain = pretrain\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.all_embeddings = tf.keras.layers.Embedding(input_dim=self.item_num + 1, output_dim=self.hidden_size, name='all_embeddings')\n",
    "        self.gru = tf.keras.layers.GRU(self.hidden_size, name='gru')\n",
    "\n",
    "        self.output1 = tf.keras.layers.Dense(self.item_num, activation=None, name=\"q-value\")\n",
    "        self.output2 = tf.keras.layers.Dense(self.item_num, activation=None, name=\"ce-logits\")\n",
    "\n",
    "    def call(self, inputs, len_state=10):\n",
    "        inputs_tensor = tf.convert_to_tensor(inputs)\n",
    "        input_emb = self.all_embeddings(inputs_tensor)\n",
    "        states_hidden = self.gru(input_emb)\n",
    "\n",
    "        q_values = self.output1(states_hidden)\n",
    "        ce_logits = self.output2(states_hidden)\n",
    "        \n",
    "        return q_values, ce_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(mainQN):\n",
    "    \n",
    "    print(\"Entered evaluate part\")\n",
    "    \n",
    "    eval_sessions = pd.read_pickle(os.path.join(data_directory, 'sampled_val.df'))\n",
    "    eval_ids = eval_sessions.session_id.unique()\n",
    "    groups = eval_sessions.groupby('session_id')\n",
    "    batch = 100\n",
    "    evaluated = 0\n",
    "    total_clicks = 0.0\n",
    "    total_purchase = 0.0\n",
    "    total_reward = [0, 0, 0, 0]\n",
    "    hit_clicks = [0, 0, 0, 0]\n",
    "    ndcg_clicks = [0, 0, 0, 0]\n",
    "    hit_purchase = [0, 0, 0, 0]\n",
    "    ndcg_purchase = [0, 0, 0, 0]\n",
    "    \n",
    "    while evaluated < len(eval_ids):\n",
    "        states, len_states, actions, rewards = [], [], [], []\n",
    "        for i in range(batch):\n",
    "            id = eval_ids[evaluated]\n",
    "            group = groups.get_group(id)\n",
    "            history = []\n",
    "            for index, row in group.iterrows():\n",
    "                state = list(history)\n",
    "                len_states.append(state_size if len(state) >= state_size else 1 if len(state) == 0 else len(state))\n",
    "                state = pad_history(state, state_size, item_num)\n",
    "                states.append(state)\n",
    "                action = row['item_id']\n",
    "                is_buy = row['is_buy']\n",
    "                reward = reward_buy if is_buy == 1 else reward_click\n",
    "                if is_buy == 1:\n",
    "                    total_purchase += 1.0\n",
    "                else:\n",
    "                    total_clicks += 1.0\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                history.append(row['item_id'])\n",
    "            evaluated += 1\n",
    "\n",
    "        _, prediction = mainQN(states)\n",
    "        sorted_list = np.argsort(prediction)\n",
    "        calculate_hit(sorted_list, topk, actions, rewards, reward_click, total_reward, hit_clicks, ndcg_clicks, hit_purchase, ndcg_purchase)\n",
    "    \n",
    "    print('#############################################################')\n",
    "    print('total clicks: %d, total purchase:%d' % (total_clicks, total_purchase))\n",
    "    for i in range(len(topk)):\n",
    "        hr_click = hit_clicks[i] / total_clicks\n",
    "        hr_purchase = hit_purchase[i] / total_purchase\n",
    "        ng_click = ndcg_clicks[i] / total_clicks\n",
    "        ng_purchase = ndcg_purchase[i] / total_purchase\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print('cumulative reward @ %d: %f' % (topk[i], total_reward[i]))\n",
    "        print('clicks hr ndcg @ %d : %f, %f' % (topk[i], hr_click, ng_click))\n",
    "        print('purchase hr and ndcg @%d : %f, %f' % (topk[i], hr_purchase, ng_purchase))\n",
    "    print('#############################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = r'E:\\MIni\\v2\\data'\n",
    "data_directory = '/media/amd/New Volume/MIni/v2/data'\n",
    "data_statis = pd.read_pickle(os.path.join(data_directory, 'data_statis.df'))\n",
    "state_size = data_statis['state_size'][0]\n",
    "item_num = data_statis['item_num'][0]\n",
    "reward_click = r_click\n",
    "reward_buy = r_buy\n",
    "topk = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 08:29:48.628117: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "QN_1 = QNetwork(hidden_size=hidden_factor, learning_rate=lr, item_num=item_num,\n",
    "                    state_size=state_size, pretrain=False)\n",
    "QN_2 = QNetwork(hidden_size=hidden_factor, learning_rate=lr, item_num=item_num,\n",
    "                    state_size=state_size, pretrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Step 100 - Loss 4.660780906677246\n",
      "Epoch 0 - Step 200 - Loss 6.779764652252197\n",
      "Epoch 0 - Step 300 - Loss 7.791237831115723\n",
      "Epoch 0 - Step 400 - Loss 37.61509704589844\n",
      "Epoch 0 - Step 500 - Loss 6.941433429718018\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = pd.read_pickle(os.path.join(data_directory, 'replay_buffer.df'))\n",
    "total_step = 0\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "num_rows=replay_buffer.shape[0]\n",
    "num_batches=int(num_rows/batch_size)\n",
    "\n",
    "#current_q_val = tf.Variable(tf.zeros([batch_size, item_num]))\n",
    "\n",
    "# optimizer.build(mainQN.trainable_variables)\n",
    "optimizer.build(QN_1.trainable_variables + QN_2.trainable_variables)  # Build optimizer with all trainable variables\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(num_batches):\n",
    "        batch = replay_buffer.sample(n=batch_size).to_dict()\n",
    "        \n",
    "        # print(batch.keys())\n",
    "        \n",
    "        state = list(batch['state'].values())\n",
    "        len_state = list(batch['len_state'].values())\n",
    "        \n",
    "        next_state = list(batch['next_state'].values())\n",
    "        len_next_state = list(batch['len_next_states'].values())\n",
    "        \n",
    "        action = list(batch['action'].values())\n",
    "        is_buy = list(batch['is_buy'].values())\n",
    "        reward = [reward_buy if is_buy[index] else reward_click for index in range(len(is_buy))]\n",
    "        \n",
    "        #This is giving raw value of discount need to change it to add a variable\n",
    "        discounts = [discount] * len(action)\n",
    "\n",
    "        pointer = np.random.randint(0, 2)\n",
    "        if pointer == 0:\n",
    "            mainQN = QN_1\n",
    "            target_QN = QN_2\n",
    "        else:\n",
    "            mainQN = QN_2\n",
    "            target_QN = QN_1\n",
    "        \n",
    "        action = tf.convert_to_tensor(action)  # Convert to TensorFlow tensor\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)  # Convert to TensorFlow tensor\n",
    "        # discount = tf.ones_like(reward) * discount\n",
    "        discounts = tf.convert_to_tensor(discounts)  # Convert to TensorFlow tensor\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            current_Qs, ce_logits = mainQN(state)\n",
    "            \n",
    "            target_Qs, _ = target_QN(next_state)\n",
    "            target_Qs_selector, _ = mainQN(next_state)\n",
    "            # print(target_Qs.shape, target_Qs_selector.shape, current_Qs.shape, ce_logits.shape, \"the shapes\")\n",
    "\n",
    "            is_done = list(batch['is_done'].values())\n",
    "            for index in range(target_Qs.shape[0]):\n",
    "                if is_done[index]:\n",
    "                    # target_Qs[index] = np.zeros(item_num)\n",
    "                    target_Qs = tf.tensor_scatter_nd_update(target_Qs, tf.expand_dims(tf.range(tf.shape(target_Qs)[0]), axis=1), tf.zeros([tf.shape(target_Qs)[0], item_num]))\n",
    "\n",
    "\n",
    "            # print(\"current_Qs:\", current_Qs.dtype, current_Qs.shape)\n",
    "            # print(\"action:\", action.dtype, action.shape)\n",
    "            # print(\"reward:\", reward.dtype, reward.shape)\n",
    "            # print(\"discount:\", discount.dtype, discount.shape)\n",
    "            # print(\"target_Qs:\", target_Qs.dtype, target_Qs.shape)\n",
    "            # print(\"target_Qs_selector:\", target_Qs_selector.dtype, target_Qs_selector.shape)\n",
    "            \n",
    "            qloss, q_learning = trfl.double_qlearning(current_Qs, action, reward, discounts,\n",
    "                                                      target_Qs, target_Qs_selector)\n",
    "            \n",
    "            q_indexed = tf.stop_gradient(indexing_ops.batched_index(current_Qs, action))\n",
    "            celoss = tf.multiply(q_indexed, tf.nn.sparse_softmax_cross_entropy_with_logits(labels=action,\n",
    "                                                                                           logits=ce_logits))\n",
    "            loss = tf.reduce_mean(qloss + celoss)\n",
    "\n",
    "        grads = tape.gradient(loss, mainQN.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, mainQN.trainable_variables))\n",
    "\n",
    "        total_step += 1\n",
    "        if total_step % 100 == 0:\n",
    "            print('Epoch {} - Step {} - Loss {}'.format(i, total_step, loss.numpy()))\n",
    "        if total_step % 1000 == 0:\n",
    "            QN_1.save_weights('DQN_1.h5')\n",
    "            QN_2.save_weights('DQN_2.h5')\n",
    "            evaluate(mainQN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
