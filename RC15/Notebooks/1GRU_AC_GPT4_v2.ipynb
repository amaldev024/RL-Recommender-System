{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 13:04:42.973723: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-06 13:04:42.973800: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-06 13:04:42.975583: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-06 13:04:42.986864: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-06 13:04:44.128079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from collections import deque\n",
    "from utility_v2 import *\n",
    "from trfl import indexing_ops\n",
    "import trfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "data = \"abc\"\n",
    "batch_size = 256\n",
    "hidden_factor = 62\n",
    "r_click = 1\n",
    "r_buy = 5\n",
    "lr = 0.01\n",
    "discount = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, learning_rate, item_num, state_size, pretrain, model_name='DQNetwork'):\n",
    "        super(QNetwork, self).__init__(name=model_name)\n",
    "        # self.state_size = state_size\n",
    "        # learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.item_num = int(item_num)\n",
    "        self.pretrain = pretrain\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.all_embeddings = tf.keras.layers.Embedding(input_dim=self.item_num + 1, output_dim=self.hidden_size, name='all_embeddings')\n",
    "        self.gru = tf.keras.layers.GRU(self.hidden_size, name='gru')\n",
    "\n",
    "\n",
    "        
    "        self.output1 = tf.keras.layers.Dense(self.item_num, activation=None, name=\"q-value\")\n",
    "        self.output2 = tf.keras.layers.Dense(self.item_num, activation=None, name=\"ce-logits\")\n",
    "\n",
    "        
    "\n",
    "    def call(self, inputs, len_state=10):\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        input_emb = self.all_embeddings(inputs)\n",
    "        states_hidden = self.gru(input_emb)\n",
    "\n",
    "        q_values = self.output1(states_hidden)\n",
    "        ce_logits = self.output2(states_hidden)\n",
    "\n",
    "        # qloss, q_learning = trfl.double_qlearning(Q_values, self.actions, self.reward, self.discount, self.targetQs_, self.targetQs_selector)\n",
    "        # q_indexed = tf.gather(Q_values, self.actions, axis=1, batch_dims=0)\n",
    "        # celoss = tf.multiply(q_indexed, tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.actions, logits=ce_logits))\n",
    "        # loss = tf.reduce_mean(qloss + celoss)\n",
    "        \n",
    "        return q_values, ce_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(QN_1):\n",
    "    eval_sessions = pd.read_pickle(os.path.join(data_directory, 'sampled_val.df'))\n",
    "    eval_ids = eval_sessions.session_id.unique()\n",
    "    groups = eval_sessions.groupby('session_id')\n",
    "    batch = 100\n",
    "    evaluated = 0\n",
    "    total_clicks = 0.0\n",
    "    total_purchase = 0.0\n",
    "    total_reward = [0, 0, 0, 0]\n",
    "    hit_clicks = [0, 0, 0, 0]\n",
    "    ndcg_clicks = [0, 0, 0, 0]\n",
    "    hit_purchase = [0, 0, 0, 0]\n",
    "    ndcg_purchase = [0, 0, 0, 0]\n",
    "    \n",
    "    while evaluated < len(eval_ids):\n",
    "        states, len_states, actions, rewards = [], [], [], []\n",
    "        for i in range(batch):\n",
    "            id = eval_ids[evaluated]\n",
    "            group = groups.get_group(id)\n",
    "            history = []\n",
    "            for index, row in group.iterrows():\n",
    "                state = list(history)\n",
    "                len_states.append(state_size if len(state) >= state_size else 1 if len(state) == 0 else len(state))\n",
    "                state = pad_history(state, state_size, item_num)\n",
    "                states.append(state)\n",
    "                action = row['item_id']\n",
    "                is_buy = row['is_buy']\n",
    "                reward = reward_buy if is_buy == 1 else reward_click\n",
    "                if is_buy == 1:\n",
    "                    total_purchase += 1.0\n",
    "                else:\n",
    "                    total_clicks += 1.0\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                history.append(row['item_id'])\n",
    "            evaluated += 1\n",
    "\n",
    "        prediction, _ = QN_1(states)\n",
    "        sorted_list = np.argsort(prediction)\n",
    "        calculate_hit(sorted_list, topk, actions, rewards, reward_click, total_reward, hit_clicks, ndcg_clicks, hit_purchase, ndcg_purchase)\n",
    "    \n",
    "    print('#############################################################')\n",
    "    print('total clicks: %d, total purchase:%d' % (total_clicks, total_purchase))\n",
    "    for i in range(len(topk)):\n",
    "        hr_click = hit_clicks[i] / total_clicks\n",
    "        hr_purchase = hit_purchase[i] / total_purchase\n",
    "        ng_click = ndcg_clicks[i] / total_clicks\n",
    "        ng_purchase = ndcg_purchase[i] / total_purchase\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print('cumulative reward @ %d: %f' % (topk[i], total_reward[i]))\n",
    "        print('clicks hr ndcg @ %d : %f, %f' % (topk[i], hr_click, ng_click))\n",
    "        print('purchase hr and ndcg @%d : %f, %f' % (topk[i], hr_purchase, ng_purchase))\n",
    "    print('#############################################################')"
   ]
  },
    {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = r'E:\\MIni\\v2\\data'\n",
    "data_directory = '/media/amd/New Volume/MIni/v2/data'\n",
    "data_statis = pd.read_pickle(os.path.join(data_directory, 'data_statis.df'))\n",
    "state_size = data_statis['state_size'][0]\n",
    "item_num = data_statis['item_num'][0]\n",
    "reward_click = r_click\n",
    "reward_buy = r_buy\n",
    "topk = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 13:04:47.138605: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QNetwork' object has no attribute 'states_hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m QN_1 \u001b[38;5;241m=\u001b[39m \u001b[43mQNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m QN_2 \u001b[38;5;241m=\u001b[39m QNetwork(hidden_size\u001b[38;5;241m=\u001b[39mhidden_factor, learning_rate\u001b[38;5;241m=\u001b[39mlr, item_num\u001b[38;5;241m=\u001b[39mitem_num,\n\u001b[1;32m      4\u001b[0m                     state_size\u001b[38;5;241m=\u001b[39mstate_size, pretrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mQNetwork.__init__\u001b[0;34m(self, hidden_size, learning_rate, item_num, state_size, pretrain, model_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mGRU(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgru\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# self.inputs = tf.keras.layers.Input(shape=(state_size,), dtype=tf.int32)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# self.len_state = tf.keras.layers.Input(shape=(), dtype=tf.int32)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# self.input_emb = tf.keras.layers.Embedding(input_dim=self.item_num + 1, output_dim=self.hidden_size)(self.inputs)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# gru_out, self.states_hidden = tf.keras.layers.GRU(self.hidden_size)(self.input_emb, mask=tf.sequence_mask(self.len_state, dtype=tf.float32))\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_num, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq-value\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates_hidden\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_num, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mce-logits\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates_hidden)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# self.actions = tf.keras.layers.Input(shape=(), dtype=tf.int32)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# self.targetQs_ = tf.keras.layers.Input(shape=(item_num,), dtype=tf.float32)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# self.targetQs_selector = tf.keras.layers.Input(shape=(item_num,), dtype=tf.float32)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# self.reward = tf.keras.layers.Input(shape=(), dtype=tf.float32)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# self.discount = tf.keras.layers.Input(shape=(), dtype=tf.float32)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QNetwork' object has no attribute 'states_hidden'"
     ]
    }
   ],
   "source": [
    "QN_1 = QNetwork(hidden_size=hidden_factor, learning_rate=lr, item_num=item_num,\n",
    "                    state_size=state_size, pretrain=False)\n",
    "QN_2 = QNetwork(hidden_size=hidden_factor, learning_rate=lr, item_num=item_num,\n",
    "                    state_size=state_size, pretrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['state', 'len_state', 'action', 'is_buy', 'next_state', 'len_next_states', 'is_done'])\n",
      "ERROR:tensorflow:DoubleQLearning: Error in rank and/or compatibility check, Shape (16, 16, 16) must have rank 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DoubleQLearning: Error in rank and/or compatibility check, Shape (16, 16, 16) must have rank 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/trfl/base_ops.py:91\u001b[0m, in \u001b[0;36mwrap_rank_shape_assert\u001b[0;34m(tensors_list, expected_ranks, op_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m tensors, rank \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors_list, expected_ranks):\n\u001b[0;32m---> 91\u001b[0m     \u001b[43massert_rank_and_shape_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/trfl/base_ops.py:84\u001b[0m, in \u001b[0;36massert_rank_and_shape_compatibility\u001b[0;34m(tensors, rank)\u001b[0m\n\u001b[1;32m     83\u001b[0m tensor_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensorShape(tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 84\u001b[0m \u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_has_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m union_of_shapes \u001b[38;5;241m=\u001b[39m union_of_shapes\u001b[38;5;241m.\u001b[39mmerge_with(tensor_shape)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/tensor_shape.py:1107\u001b[0m, in \u001b[0;36mTensorShape.assert_has_rank\u001b[0;34m(self, rank)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, rank):\n\u001b[0;32m-> 1107\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have rank \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m, rank))\n",
      "\u001b[0;31mValueError\u001b[0m: Shape (16, 16, 16) must have rank 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 61\u001b[0m\n\u001b[1;32m     54\u001b[0m target_Qs_selector \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(target_Qs_selector)  \u001b[38;5;66;03m# Convert to TensorFlow tensor\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 61\u001b[0m     qloss, q_learning \u001b[38;5;241m=\u001b[39m \u001b[43mtrfl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_qlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_Qs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mtarget_Qs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_Qs_selector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     q_indexed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstop_gradient(indexing_ops\u001b[38;5;241m.\u001b[39mbatched_index(current_Qs, action))\n\u001b[1;32m     64\u001b[0m     celoss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmultiply(q_indexed, tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msparse_softmax_cross_entropy_with_logits(labels\u001b[38;5;241m=\u001b[39maction,\n\u001b[1;32m     65\u001b[0m                                                                                    logits\u001b[38;5;241m=\u001b[39mce_logits))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/trfl/action_value_ops.py:120\u001b[0m, in \u001b[0;36mdouble_qlearning\u001b[0;34m(q_tm1, a_tm1, r_t, pcont_t, q_t_value, q_t_selector, name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implements the double Q-learning loss as a TensorFlow op.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03mThe loss is `0.5` times the squared difference between `q_tm1[a_tm1]` and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m      * `best_action`: batch of greedy actions wrt `q_t_selector`, shape `[B]`\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Rank and compatibility checks.\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43mbase_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_rank_shape_assert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq_tm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_t_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_t_selector\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43ma_tm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcont_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# double Q-learning op.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\n\u001b[1;32m    125\u001b[0m     name, values\u001b[38;5;241m=\u001b[39m[q_tm1, a_tm1, r_t, pcont_t, q_t_value, q_t_selector]):\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m   \u001b[38;5;66;03m# Build target and select head to update.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/trfl/base_ops.py:96\u001b[0m, in \u001b[0;36mwrap_rank_shape_assert\u001b[0;34m(tensors_list, expected_ranks, op_name)\u001b[0m\n\u001b[1;32m     93\u001b[0m error_message \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: Error in rank and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompatibility check, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(op_name, e))\n\u001b[1;32m     95\u001b[0m tf\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39merror(error_message)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n",
      "\u001b[0;31mValueError\u001b[0m: DoubleQLearning: Error in rank and/or compatibility check, Shape (16, 16, 16) must have rank 1"
     ]
    }
   ],
   "source": [
    "replay_buffer = pd.read_pickle(os.path.join(data_directory, 'replay_buffer.df'))\n",
    "total_step = 0\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "num_rows=replay_buffer.shape[0]\n",
    "num_batches=int(num_rows/batch_size)\n",
    "\n",
    "#current_q_val = tf.Variable(tf.zeros([batch_size, item_num]))\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(num_batches):\n",
    "        batch = replay_buffer.sample(n=batch_size).to_dict()\n",
    "        \n",
    "        print(batch.keys())\n",
    "        \n",
    "        state = list(batch['state'].values())\n",
    "        len_state = list(batch['len_state'].values())\n",
    "        \n",
    "        next_state = list(batch['next_state'].values())\n",
    "        len_next_state = list(batch['len_next_states'].values())\n",
    "\n",
    "        pointer = np.random.randint(0, 2)\n",
    "        if pointer == 0:\n",
    "            mainQN = QN_1\n",
    "            target_QN = QN_2\n",
    "        else:\n",
    "            mainQN = QN_2\n",
    "            target_QN = QN_1\n",
    "            \n",
    "        current_Qs, ce_logits = mainQN(state)\n",
    "        \n",
    "        target_Qs, _ = target_QN(next_state)\n",
    "        target_Qs_selector, _ = mainQN(next_state)\n",
    "\n",
    "        is_done = list(batch['is_done'].values())\n",
    "        for index in range(target_Qs.shape[0]):\n",
    "            if is_done[index]:\n",
    "                # target_Qs[index] = np.zeros(item_num)\n",
    "                target_Qs = tf.tensor_scatter_nd_update(target_Qs, tf.expand_dims(tf.range(tf.shape(target_Qs)[0]), axis=1), tf.zeros([tf.shape(target_Qs)[0], item_num]))\n",
    "\n",
    "        \n",
    "        action = list(batch['action'].values())\n",
    "        is_buy = list(batch['is_buy'].values())\n",
    "        reward = [reward_buy if is_buy[index] else reward_click for index in range(len(is_buy))]\n",
    "        discount = [discount] * len(action)\n",
    "        \n",
    "        \n",
    "        current_Qs = tf.convert_to_tensor(current_Qs)  # Convert to TensorFlow tensor\n",
    "        action = tf.convert_to_tensor(action)  # Convert to TensorFlow tensor\n",
    "        reward = tf.convert_to_tensor(reward)  # Convert to TensorFlow tensor\n",
    "        discount = tf.convert_to_tensor(discount)  # Convert to TensorFlow tensor\n",
    "        target_Qs = tf.convert_to_tensor(target_Qs)  # Convert to TensorFlow tensor\n",
    "        target_Qs_selector = tf.convert_to_tensor(target_Qs_selector)  # Convert to TensorFlow tensor\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            qloss, q_learning = trfl.double_qlearning(current_Qs, action, reward, discount,\n",
    "                                                      target_Qs, target_Qs_selector)\n",
    "            q_indexed = tf.stop_gradient(indexing_ops.batched_index(current_Qs, action))\n",
    "            celoss = tf.multiply(q_indexed, tf.nn.sparse_softmax_cross_entropy_with_logits(labels=action,\n",
    "                                                                                           logits=ce_logits))\n",
    "            loss = tf.reduce_mean(qloss + celoss)\n",
    "\n",
    "        grads = tape.gradient(loss, mainQN.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, mainQN.trainable_variables))\n",
    "\n",
    "        current_q_val = target_Qs\n",
    "        total_step += 1\n",
    "        if total_step % 100 == 0:\n",
    "            print('Epoch {} - Step {} - Loss {}'.format(i, total_step, loss.numpy()))\n",
    "        if total_step % 1000 == 0:\n",
    "            QN_1.save_weights('DQN_1.h5')\n",
    "            QN_2.save_weights('DQN_2.h5')\n",
    "            evaluate(QN_1)\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data:\n",
      "[[0.04861772 0.21494646 0.1372986  0.6747829  0.91644514 0.69690466\n",
      "  0.45919695 0.31513068 0.04708688 0.94342834 0.7108823  0.29495445\n",
      "  0.19612813 0.56713516 0.29556203 0.8914198 ]]\n",
      "Model Output:\n",
      "tf.Tensor(\n",
      "[[-0.23957685  0.13985115 -0.08043909  0.16449688 -0.18504213 -0.30274737\n",
      "  -0.00161541  0.33442557 -0.18033543 -0.20127648]], shape=(1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define the QNetwork class\n",
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Create an instance of the QNetwork\n",
    "qn = QNetwork()\n",
    "\n",
    "# Generate some sample input data (you can replace this with your actual data)\n",
    "input_data = np.random.rand(1, 16).astype(np.float32)\n",
    "\n",
    "# Pass the input data to the QNetwork\n",
    "output = qn(input_data)\n",
    "\n",
    "# Print the output\n",
    "print(\"Input Data:\")\n",
    "print(input_data)\n",
    "print(\"Model Output:\")\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
